{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing Tutorials\n",
    "\n",
    "This notebook walks through common NLP tasks using Python libraries.\n",
    "We install required packages, explore tokenization, create a bag-of-words\n",
    "representation, train a simple classifier, and use pre-trained models for\n",
    "sentiment analysis, named entity recognition, and word embeddings. Each\n",
    "section explains what the code is doing under the hood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nltk scikit-learn transformers spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import classification_report\n",
    "from transformers import pipeline\n",
    "import spacy\n",
    "from spacy.cli import download"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Tokenize text with NLTK\n",
    "\n",
    "Tokenization is the process of breaking raw text into smaller units such as\n",
    "words or punctuation symbols. NLTK provides language specific tokenizers\n",
    "that contain rules for splitting text. Here we download the tokenizer\n",
    "data and apply it to a short sentence to obtain a list of word tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt_tab')\n",
    "text = 'Natural language processing with Python is fun!'\n",
    "tokens = nltk.word_tokenize(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Bag-of-words representation\n",
    "\n",
    "A bag-of-words model encodes each document as a vector of token counts.\n",
    "`CountVectorizer` builds a vocabulary mapping every unique word to an index\n",
    "and then counts how often those words occur in each document. The resulting\n",
    "sparse matrix can be used as input to machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = ['I love coding in Python', 'Python can be used for NLP']\n",
    "vectorizer = CountVectorizer()\n",
    "bag = vectorizer.fit_transform(docs)\n",
    "print('Vocabulary:', vectorizer.vocabulary_)\n",
    "print('Bag-of-words matrix:', bag.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Train a text classifier\n",
    "\n",
    "We fetch two categories from the 20 Newsgroups dataset (baseball and space).\n",
    "The text is converted to a bag-of-words representation and a logistic\n",
    "regression model is trained to distinguish the topics. After fitting, we\n",
    "evaluate on the held-out test split and print a classification report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = ['rec.sport.baseball', 'sci.space']\n",
    "train = fetch_20newsgroups(subset='train', categories=categories, remove=('headers','footers','quotes'))\n",
    "test = fetch_20newsgroups(subset='test', categories=categories, remove=('headers','footers','quotes'))\n",
    "clf = make_pipeline(CountVectorizer(), LogisticRegression(max_iter=1000))\n",
    "clf.fit(train.data, train.target)\n",
    "preds = clf.predict(test.data)\n",
    "print(classification_report(test.target, preds, target_names=test.target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Sentiment analysis with transformers\n",
    "\n",
    "The Hugging Face `pipeline` API downloads a pretrained transformer model\n",
    "that can detect positive or negative sentiment. We pass a sentence to the\n",
    "pipeline and it returns the predicted label and confidence score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment = pipeline('sentiment-analysis')\n",
    "result = sentiment('I love using transformers for NLP!')[0]\n",
    "print(f\"Label: {result['label']}, score: {result['score']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Named entity recognition with spaCy\n",
    "\n",
    "spaCy ships with pretrained models for many languages. Loading the English\n",
    "model gives access to a statistical parser that can identify names,\n",
    "organizations and locations in text. We process a sample sentence and\n",
    "loop over the detected entities to print their text and type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "except OSError:\n",
    "    download('en_core_web_sm')\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp('Apple was founded by Steve Jobs in California.')\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Word embeddings and similarity\n",
    "\n",
    "spaCy's medium model includes word vectors that capture semantic meaning.\n",
    "By comparing the cosine similarity between vectors we can quantify how\n",
    "similar two words are. The example loads the vectors and prints the\n",
    "pairwise similarity scores for a few tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    nlp_md = spacy.load('en_core_web_md')\n",
    "except OSError:\n",
    "    download('en_core_web_md')\n",
    "    nlp_md = spacy.load('en_core_web_md')\n",
    "tokens = nlp_md('dog cat banana')\n",
    "for t1 in tokens:\n",
    "    for t2 in tokens:\n",
    "        print(f'Similarity({t1.text}, {t2.text}) = {t1.similarity(t2):.3f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cogs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}