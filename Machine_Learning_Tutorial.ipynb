{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Machine Learning with Python\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction to Machine Learning\n",
    "Machine learning is about building systems that can learn from data. Common types of\n",
    "learning include:\n",
    "- **Supervised learning**: learning from labeled examples (regression and classification).\n",
    "- **Unsupervised learning**: finding patterns in unlabeled data.\n",
    "- **Reinforcement learning**: learning by interacting with an environment.\n",
    "\n",
    "In this notebook we'll focus on supervised learning (both regression and classification)\n",
    "and briefly look at unsupervised learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setting Up the Environment\n",
    "Below we import the libraries we'll use. If you don't have them installed, you can\n",
    "install them using `pip`. NumPy provides efficient array operations, pandas offers\n",
    "data manipulation tools, matplotlib is for plotting, and scikit-learn has many ML\n",
    "algorithms ready to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install packages if needed (uncomment the following lines)\n",
    "# !pip install numpy pandas matplotlib scikit-learn\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets, model_selection, metrics, linear_model, neighbors, tree, cluster\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A quick word on Jupyter Notebooks: each cell can contain code or Markdown. You can\n",
    "execute a code cell with `Shift+Enter`. It's good practice to keep related code in\n",
    "small cells and to restart the kernel occasionally to make sure your notebook runs\n",
    "from a clean state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Gentle Introduction to NumPy\n",
    "NumPy is a foundational package for numerical computing in Python. ",
    "The core object is the **array**, which behaves similarly to a list but stores data in a contiguous block of memory. ",
    "This allows operations on entire arrays to be performed very quickly in compiled code. ",
    "We'll start by creating some arrays and exploring indexing, basic arithmetic, and the concept of *broadcasting* which lets NumPy apply operations to arrays of different shapes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating arrays\n",
    "a = np.array([1, 2, 3])\n",
    "b = np.array([[1, 2, 3], [4, 5, 6]])\n",
    "print(\"a:\", a)\n",
    "print(\"b:\n",
    "\", b)\n",
    "\n",
    "# Indexing and slicing\n",
    "print(\"a[0] =\", a[0])\n",
    "print(\"b[0, 1] =\", b[0, 1])\n",
    "print(\"b[:, 1] =\", b[:, 1])\n",
    "\n",
    "# Vectorized operations\n",
    "print(\"a * 2 =\", a * 2)\n",
    "print(\"a + 3 =\", a + 3)\n",
    "print(\"a + a =\", a + a)\n",
    "\n",
    "# Broadcasting example\n",
    "c = np.array([1, 2, 3])\n",
    "d = np.array([[10], [20], [30]])\n",
    "print(\"c + d =\n",
    "\", c + d)\n",
    "\n",
    "# Dot product\n",
    "print(\"Dot product a\u00b7c =\", np.dot(a, c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output above illustrates several important ideas:\n",
    "- `a` is a one-dimensional array while `b` is two-dimensional. The shape of an array is accessed via the `shape` attribute.\n",
    "- Indexing works with square brackets, e.g. `b[0, 1]` fetches the element in the first row and second column.\n",
    "- Operations like `a * 2` or `a + 3` automatically apply element-wise to every entry.\n",
    "- When arrays have compatible shapes, NumPy *broadcasts* them so the arithmetic still works\u2014notice how `c + d` adds a 1-D array to a column vector.\n",
    "- `np.dot` computes the dot product without an explicit Python loop.\n",
    "\n",
    "Take a moment to execute the cell and observe how each print statement matches these explanations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hands-on Challenge\n",
    "Implement the dot product of two vectors using a for loop. Fill in the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def manual_dot(x, y):\n",
    "    \"\"\"Compute dot product of two 1-D arrays x and y using a loop.\"\"\"\n",
    "    # TODO: replace with your implementation\n",
    "    result = 0.0\n",
    "    for i in range(len(x)):\n",
    "        pass  # replace this line\n",
    "    return result\n",
    "\n",
    "manual_dot(np.array([1, 2, 3]), np.array([4, 5, 6]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Linear Regression (Supervised Learning: Regression)\n",
    "Linear regression attempts to fit a straight line through the data.\n",
    "For an input vector $x$ we predict a scalar $y$ using $\\hat{y} = w^Tx + b$.\n",
    "The parameters $w$ and $b$ are chosen to minimize the mean squared error (MSE) between the predictions and the true targets.\n",
    "We will generate a simple synthetic dataset and then optimize the parameters with **gradient descent**\u2014an iterative algorithm that nudges the parameters in the direction that most reduces the error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "X = 2 * np.random.rand(100, 1)\n",
    "y = 4 + 3 * X[:,0] + np.random.randn(100)\n",
    "X_b = np.c_[np.ones((X.shape[0], 1)), X]\n",
    "eta = 0.1\n",
    "n_iterations = 1000\n",
    "theta = np.random.randn(2)\n",
    "mse_history = []\n",
    "for iteration in range(n_iterations):\n",
    "    gradients = 2/len(X_b) * X_b.T.dot(X_b.dot(theta) - y)\n",
    "    theta -= eta * gradients\n",
    "    mse_history.append(np.mean((X_b.dot(theta) - y)**2))\n",
    "print(\"Parameters:\", theta)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the loop above:\n",
    "1. We compute the predictions `X_b.dot(theta)` using the current parameters.\n",
    "2. The gradient of the MSE tells us how to change `theta` to reduce the error.\n",
    "3. We subtract a small fraction (`eta`) of this gradient from `theta` each iteration.\n",
    "\n",
    "Over many iterations the parameters converge and `mse_history` tracks the error at each step."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(mse_history)\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('MSE')\n",
    "plt.title('Gradient Descent Convergence')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,4))\n",
    "plt.scatter(X, y, label=\"Data\")\n",
    "plt.plot(X, X_b.dot(theta), color=\"red\", label=\"Model\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg = linear_model.LinearRegression()\n",
    "lin_reg.fit(X, y)\n",
    "print(\"scikit-learn parameters:\", [lin_reg.intercept_, lin_reg.coef_[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hands-on Challenge\n",
    "Modify the learning rate `eta` above and re-run the gradient descent loop. Observe\n",
    "how it affects convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Logistic Regression (Supervised Learning: Classification)\n",
    "While linear regression predicts a continuous value, logistic regression predicts the probability that an example belongs to class 1.\n",
    "It uses the **sigmoid** function to squeeze the output of a linear model between 0 and 1:\n",
    "$$\\hat{y} = \\sigma(w^Tx + b) = \\frac{1}{1 + e^{-(w^Tx + b)}}.$$\n",
    "We'll create a small synthetic dataset with two features and then train a logistic regression model with gradient descent, similar to how we optimized the linear regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "X, y = make_classification(n_features=2, n_redundant=0, n_informative=2, n_clusters_per_class=1, random_state=0)\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.scatter(X[:,0], X[:,1], c=y, cmap='bwr', edgecolor='k')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.title('Synthetic classification dataset')\n",
    "plt.show()\n",
    "X_b = np.c_[np.ones((X.shape[0], 1)), X]\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "eta = 0.1\n",
    "n_iterations = 1000\n",
    "theta = np.random.randn(X_b.shape[1])\n",
    "for _ in range(n_iterations):\n",
    "    scores = X_b.dot(theta)\n",
    "    predictions = sigmoid(scores)\n",
    "    gradient = X_b.T.dot(predictions - y) / len(y)\n",
    "    theta -= eta * gradient\n",
    "print('Parameters:', theta)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we repeatedly:\n",
    "1. Compute the linear scores `X_b.dot(theta)` for each sample.\n",
    "2. Apply the sigmoid function to obtain probabilities.\n",
    "3. Update `theta` using the gradient of the cross-entropy loss.\n",
    "\n",
    "After training we can classify new points by checking if the predicted probability is at least 0.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = sigmoid(X_b.dot(theta))\n",
    "y_pred = (probs >= 0.5).astype(int)\n",
    "print(\"Accuracy:\", (y_pred == y).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0_min, x0_max = X[:,0].min() - .5, X[:,0].max() + .5\n",
    "x1_min, x1_max = X[:,1].min() - .5, X[:,1].max() + .5\n",
    "xx0, xx1 = np.meshgrid(np.linspace(x0_min, x0_max, 200), np.linspace(x1_min, x1_max, 200))\n",
    "X_grid = np.c_[np.ones((xx0.size, 1)), xx0.ravel(), xx1.ravel()]\n",
    "probs = sigmoid(X_grid.dot(theta)).reshape(xx0.shape)\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.contourf(xx0, xx1, probs, levels=[0,0.5,1], alpha=0.3)\n",
    "plt.scatter(X[:,0], X[:,1], c=y, edgecolor=\"k\")\n",
    "plt.xlabel(\"Feature 1\")\n",
    "plt.ylabel(\"Feature 2\")\n",
    "plt.title(\"Logistic Regression Decision Boundary\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg = linear_model.LogisticRegression()\n",
    "log_reg.fit(X, y)\n",
    "print(\"scikit-learn accuracy:\", log_reg.score(X, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Digits Dataset Example\n",
    "The `sklearn` digits dataset contains 8\u00d78 images of handwritten digits. Each image is flattened into a 64-element vector.\n",
    "We'll train a logistic regression classifier on this dataset and visualize a few of the digits."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "digits = load_digits()\n",
    "X_digits = digits.data\n",
    "y_digits = digits.target\n",
    "fig, axes = plt.subplots(1, 10, figsize=(10,2))\n",
    "for i, ax in enumerate(axes):\n",
    "    ax.imshow(digits.images[i], cmap='gray')\n",
    "    ax.set_axis_off()\n",
    "    ax.set_title(str(digits.target[i]))\n",
    "plt.suptitle('Sample digits')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "log_reg_digits = linear_model.LogisticRegression(max_iter=1000)\n",
    "X_train_d, X_test_d, y_train_d, y_test_d = model_selection.train_test_split(X_digits, y_digits, test_size=0.2, random_state=0)\n",
    "log_reg_digits.fit(X_train_d, y_train_d)\n",
    "y_pred_d = log_reg_digits.predict(X_test_d)\n",
    "print('Test accuracy:', metrics.accuracy_score(y_test_d, y_pred_d))\n",
    "metrics.ConfusionMatrixDisplay.from_predictions(y_test_d, y_pred_d, cmap='Blues')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hands-on Challenge\n",
    "Implement a function `predict` that returns class labels (0 or 1) using the learned\n",
    "`theta` and test it on the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X_new):\n",
    "    \"\"\"Return 0 or 1 predictions using global theta.\"\"\"\n",
    "    # TODO: implement prediction using sigmoid\n",
    "    pass\n",
    "\n",
    "predict(X[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. k-Nearest Neighbors (Supervised Learning)\n",
    "The k-NN algorithm stores the training data and classifies a new point by looking at\n",
    "the $k$ closest training examples. We'll implement a simple version using Euclidean\n",
    "distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "class KNNClassifier:\n",
    "    def __init__(self, k=3):\n",
    "        self.k = k\n",
    "    def fit(self, X, y):\n",
    "        self.X_train = X\n",
    "        self.y_train = y\n",
    "    def _distance(self, a, b):\n",
    "        return np.linalg.norm(a - b)\n",
    "    def predict(self, X):\n",
    "        preds = []\n",
    "        for x in X:\n",
    "            distances = [self._distance(x, x_train) for x_train in self.X_train]\n",
    "            k_idx = np.argsort(distances)[:self.k]\n",
    "            k_votes = self.y_train[k_idx]\n",
    "            preds.append(Counter(k_votes).most_common(1)[0][0])\n",
    "        return np.array(preds)\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "knn = KNNClassifier(k=5)\n",
    "knn.fit(X, y)\n",
    "y_pred = knn.predict(X)\n",
    "print(\"Accuracy:\", (y_pred == y).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sk_knn = neighbors.KNeighborsClassifier(n_neighbors=5)\n",
    "sk_knn.fit(X, y)\n",
    "print(\"scikit-learn accuracy:\", sk_knn.score(X, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hands-on Challenge\n",
    "Modify the `_distance` method to use Manhattan distance instead of Euclidean."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Decision Trees (Supervised Learning)\n",
    "Decision trees split the data recursively based on feature values. We'll implement a\n",
    "simple decision stump (a tree with one split) using Gini impurity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gini_impurity(y):\n",
    "    m = len(y)\n",
    "    return 1 - sum((np.sum(y==c)/m)**2 for c in np.unique(y))\n",
    "\n",
    "def best_split(X, y):\n",
    "    m, n = X.shape\n",
    "    best_feature, best_thresh, best_gini = None, None, 1\n",
    "    for feature in range(n):\n",
    "        thresholds = np.unique(X[:, feature])\n",
    "        for t in thresholds:\n",
    "            left = y[X[:, feature] <= t]\n",
    "            right = y[X[:, feature] > t]\n",
    "            gini = (len(left)*gini_impurity(left) + len(right)*gini_impurity(right)) / m\n",
    "            if gini < best_gini:\n",
    "                best_gini = gini\n",
    "                best_feature = feature\n",
    "                best_thresh = t\n",
    "    return best_feature, best_thresh\n",
    "\n",
    "feature, thresh = best_split(X, y)\n",
    "print(\"Best feature:\", feature, \"Threshold:\", thresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,4))\n",
    "for target in np.unique(y):\n",
    "    subset = X[y==target]\n",
    "    plt.scatter(subset[:, feature], subset[:, 2], label=iris.target_names[target])\n",
    "plt.axvline(thresh, color=\"red\", linestyle=\"--\", label=\"split\")\n",
    "plt.xlabel(iris.feature_names[feature])\n",
    "plt.ylabel(iris.feature_names[2])\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sk_tree = tree.DecisionTreeClassifier(max_depth=1)\n",
    "sk_tree.fit(X, y)\n",
    "print(\"scikit-learn depth:\", sk_tree.get_depth())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hands-on Challenge\n",
    "Using the best feature and threshold found above, write code to classify new points\n",
    "with this decision stump."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stump_predict(X_new):\n",
    "    \"\"\"Return class 0/1/2 using the stump parameters.\"\"\"\n",
    "    # TODO: implement using global `feature` and `thresh`\n",
    "    pass\n",
    "\n",
    "stump_predict(X[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Brief Introduction to Unsupervised Learning\n",
    "Unsupervised learning deals with data without explicit labels. Two common tasks are\n",
    "clustering (grouping similar points) and dimensionality reduction. We'll look at the\n",
    "k-Means clustering algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "X_blobs, y_blobs = datasets.make_blobs(n_samples=200, centers=3, random_state=42)\n",
    "plt.scatter(X_blobs[:,0], X_blobs[:,1], c=y_blobs, cmap=\"viridis\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmeans(X, k, n_iters=100):\n",
    "    rng = np.random.default_rng(seed=0)\n",
    "    centroids = X[rng.choice(len(X), k, replace=False)]\n",
    "    for _ in range(n_iters):\n",
    "        distances = np.linalg.norm(X[:, None] - centroids[None, :], axis=2)\n",
    "        labels = np.argmin(distances, axis=1)\n",
    "        new_centroids = np.array([X[labels==i].mean(axis=0) for i in range(k)])\n",
    "        if np.allclose(centroids, new_centroids):\n",
    "            break\n",
    "        centroids = new_centroids\n",
    "    return centroids, labels\n",
    "\n",
    "centroids, labels = kmeans(X_blobs, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X_blobs[:,0], X_blobs[:,1], c=labels, cmap=\"viridis\", alpha=0.6)\n",
    "plt.scatter(centroids[:,0], centroids[:,1], c=\"red\", marker=\"x\", s=100)\n",
    "plt.title(\"k-Means Clustering\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sk_kmeans = cluster.KMeans(n_clusters=3, n_init=10)\n",
    "sk_kmeans.fit(X_blobs)\n",
    "print(\"scikit-learn inertia:\", sk_kmeans.inertia_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hands-on Challenge\n",
    "Add a stopping criterion to `kmeans` based on the change in centroids between\n",
    "iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Model Evaluation and Cross-Validation\n",
    "When building models, we typically split data into training and test sets (and\n",
    "sometimes a validation set). scikit-learn provides `train_test_split` and utilities\n",
    "for cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.2, random_state=42)\n",
    "clf = neighbors.KNeighborsClassifier()\n",
    "clf.fit(X_train, y_train)\n",
    "print(\"Test accuracy:\", clf.score(X_test, y_test))\n",
    "scores = cross_val_score(clf, iris.data, iris.target, cv=5)\n",
    "print(\"Cross-validation scores:\", scores)\n",
    "print(\"Mean CV score:\", scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Wrap-Up and Next Steps\n",
    "In this notebook you implemented linear and logistic regression, k-NN, a decision\n",
    "stump, and k-Means from scratch, and you compared them with scikit-learn's\n",
    "implementations. We also looked at model evaluation techniques.\n",
    "\n",
    "To go further, consider exploring:\n",
    "- The perceptron algorithm and how it leads to neural networks.\n",
    "- Regularization techniques.\n",
    "- More complex datasets and competitions such as Kaggle.\n",
    "\n",
    "Machine learning is a vast field\u2014keep experimenting and practicing!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}