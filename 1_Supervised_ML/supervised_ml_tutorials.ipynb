{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Machine Learning Tutorials\n",
    "\n",
    "This notebook walks through simple scikit-learn examples step by step.\n",
    "\n",
    "Run the cell below if you need to install the required libraries. In Google Colab they come pre-installed.\n",
    "\n",
    "In supervised learning we observe pairs $(x, y)$ and seek a function $f(x;\theta)$ that predicts $y$ from $x$.\n",
    "Each algorithm solves an optimization problem to find parameters $\theta$ that minimize a chosen loss function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install scikit-learn matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_regression, load_iris, load_digits\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, ConfusionMatrixDisplay\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Linear regression on synthetic data\n",
    "\n",
    "Linear regression assumes a linear relationship between the feature $x$ and target $y$: \n",
    "\n",
    "$$y = w x + b + \\epsilon.$$\n",
    "\n",
    "Here we generate samples $x_i$ from a normal distribution and add Gaussian noise $\\epsilon_i$ to the targets.\n",
    "Using the least-squares criterion we recover the weight $w$ and bias $b$ that minimize\n",
    "$$J(w,b)=\\sum_i (y_i - w x_i - b)^2.$$\n",
    "In matrix form the solution is $(\\hat w, \\hat b)= (X^T X)^{-1}X^T y$ where $X$ includes a column of ones for the bias.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, coef = make_regression(n_samples=100, n_features=1, noise=10.0, coef=True, random_state=42)\n",
    "print('X shape:', X.shape)\n",
    "print('y shape:', y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X, y, color='blue', label='Data')\n",
    "plt.xlabel('Feature')\n",
    "plt.ylabel('Target')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression()\n",
    "model.fit(X, y)\n",
    "print('True coefficient:', coef)\n",
    "print('Learned coefficient:', model.coef_[0])\n",
    "print('Intercept:', model.intercept_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_grid = np.linspace(X.min(), X.max(), 100).reshape(-1,1)\n",
    "y_pred = model.predict(x_grid)\n",
    "print('Prediction shape:', y_pred.shape)\n",
    "plt.scatter(X, y, color='blue', label='Data')\n",
    "plt.plot(x_grid, y_pred, color='red', label='Fit')\n",
    "plt.xlabel('Feature')\n",
    "plt.ylabel('Target')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Logistic regression on Iris\n",
    "\n",
    "Logistic regression models the probability of each class. For binary output the model uses the sigmoid\n",
    "$$P(y=1\\mid x)=\\sigma(w^T x+b)=\\frac{1}{1+e^{-w^T x-b}}.$$\n",
    "More generally, the multi-class version applies a softmax over the linear scores.\n",
    "Training maximizes the data likelihood which is equivalent to minimizing the cross-entropy loss.\n",
    "We split the Iris dataset into train and test sets and measure classification accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = load_iris(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "print('Train shape:', X_train.shape, y_train.shape)\n",
    "print('Test shape:', X_test.shape, y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression(max_iter=200)\n",
    "clf.fit(X_train, y_train)\n",
    "preds = clf.predict(X_test)\n",
    "print('Accuracy:', accuracy_score(y_test, preds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ConfusionMatrixDisplay.from_predictions(y_test, preds)\n",
    "plt.title('Logistic Regression Confusion Matrix')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. k-NN classification on digits\n",
    "\n",
    "The $k$-nearest neighbors method stores all training examples. For a new sample $x$ we compute the Euclidean\n",
    "distance to every training point, $$d(x, x_i)=\\sqrt{\\|x-x_i\\|^2}.$$\n",
    "The $k$ points with smallest distance form the neighborhood $N_k(x)$ and the predicted label is the most common\n",
    "among them: $$\\hat y=\\operatorname{mode}(\\{y_i : x_i \\in N_k(x)\\}).$$\n",
    "This lazy approach has no training phase beyond storing the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = load_digits(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "print('Train shape:', X_train.shape)\n",
    "print('Test shape:', X_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "knn.fit(X_train, y_train)\n",
    "preds = knn.predict(X_test)\n",
    "print('Test accuracy:', accuracy_score(y_test, preds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ConfusionMatrixDisplay.from_predictions(y_test, preds)\n",
    "plt.title('k-NN Confusion Matrix')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Decision tree classifier\n",
    "\n",
    "Decision trees build a hierarchy of if-else rules. At each node we choose the feature and threshold that maximize\n",
    "the reduction in impurity, often measured by the Gini index\n",
    "$$G=1-\\sum_k p_k^2,$$\n",
    "where $p_k$ is the proportion of class $k$ in that node. Splits continue until nodes become pure or a maximum depth\n",
    "is reached. The resulting tree makes predictions by following the learned rules from root to leaf.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = load_iris(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "print('Train shape:', X_train.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
    "tree.fit(X_train, y_train)\n",
    "preds = tree.predict(X_test)\n",
    "print(classification_report(y_test, preds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ConfusionMatrixDisplay.from_predictions(y_test, preds)\n",
    "plt.title('Decision Tree Confusion Matrix')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This concludes the brief tour of basic machine learning examples using scikit-learn.\n",
    "Each model optimizes a mathematical objective or rule to map inputs to outputs.\n",
    "Feel free to modify the code cells and explore further, trying different datasets or algorithms.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cogs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}