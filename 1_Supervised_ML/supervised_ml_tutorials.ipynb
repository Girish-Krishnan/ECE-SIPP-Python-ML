{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c674e36c",
   "metadata": {},
   "source": [
    "# Supervised Learning: from scratch and with scikit-learn\n",
    "\n",
    "Examples showing the math behind basic algorithms and their scikit-learn counterparts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "276594dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install numpy scikit-learn matplotlib -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9e5246",
   "metadata": {},
   "source": [
    "Scikit-learn is a powerful library for machine learning in Python, providing efficient tools for data mining and data analysis. It is built on NumPy, SciPy, and matplotlib.\n",
    "\n",
    "Documentation: [scikit-learn documentation](https://scikit-learn.org/stable/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a3e91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_regression, make_classification, load_digits, load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression, Perceptron, LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "391b54f8",
   "metadata": {},
   "source": [
    "This cell imports `numpy` for numerical arrays, utilities from `scikit-learn` for datasets and models, and `matplotlib` for plotting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afcef169",
   "metadata": {},
   "source": [
    "## 0. Linear regression from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab0be56",
   "metadata": {},
   "source": [
    "Linear regression seeks to model the relationship between a set of input features $x \\in \\mathbb{R}^d$ and a target variable $y \\in \\mathbb{R}$ by fitting a linear function:\n",
    "\n",
    "$$\n",
    "y = w^T x + b\n",
    "$$\n",
    "\n",
    "where $w \\in \\mathbb{R}^d$ is the vector of weights (coefficients) and $b \\in \\mathbb{R}$ is the intercept (bias).\n",
    "\n",
    "Given a dataset of $n$ samples $\\{(x^{(i)}, y^{(i)})\\}_{i=1}^n$, we can write the model for all samples in matrix form. Let $X \\in \\mathbb{R}^{n \\times d}$ be the matrix whose rows are the input vectors $x^{(i)}$, and $y \\in \\mathbb{R}^n$ be the vector of targets.\n",
    "\n",
    "To include the intercept $b$ in the model, we augment $X$ with a column of ones:\n",
    "\n",
    "$$\n",
    "X_b = \\begin{bmatrix} 1 & x_1^{(1)} & \\cdots & x_d^{(1)} \\\\\n",
    "1 & x_1^{(2)} & \\cdots & x_d^{(2)} \\\\\n",
    "\\vdots & \\vdots & & \\vdots \\\\\n",
    "1 & x_1^{(n)} & \\cdots & x_d^{(n)} \\end{bmatrix} \\in \\mathbb{R}^{n \\times (d+1)}\n",
    "$$\n",
    "\n",
    "and define the parameter vector\n",
    "\n",
    "$$\n",
    "\\theta = \\begin{bmatrix} b \\\\ w_1 \\\\ \\vdots \\\\ w_d \\end{bmatrix} \\in \\mathbb{R}^{d+1}\n",
    "$$\n",
    "\n",
    "The model becomes:\n",
    "\n",
    "$$\n",
    "y \\approx X_b \\theta\n",
    "$$\n",
    "\n",
    "The goal is to find $\\theta$ that minimizes the sum of squared errors (the least-squares criterion):\n",
    "\n",
    "$$\n",
    "L(\\theta) = \\|y - X_b \\theta\\|^2 = (y - X_b \\theta)^T (y - X_b \\theta)\n",
    "$$\n",
    "\n",
    "To find the minimum, we set the gradient with respect to $\\theta$ to zero:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\theta} = -2 X_b^T (y - X_b \\theta) = 0\n",
    "$$\n",
    "\n",
    "Solving for $\\theta$ gives the closed-form solution:\n",
    "\n",
    "$$\n",
    "X_b^T X_b \\theta = X_b^T y \\\\\n",
    "\\implies \\theta = (X_b^T X_b)^{-1} X_b^T y\n",
    "$$\n",
    "\n",
    "This formula computes the optimal weights and intercept that best fit the data in the least-squares sense, assuming $X_b^T X_b$ is invertible. In summary, linear regression finds the hyperplane in $\\mathbb{R}^d$ that minimizes the squared distance to all data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d426fe5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, true_coef = make_regression(n_samples=100, n_features=1, noise=10.0, coef=True, random_state=42, bias=0)\n",
    "print(\"True coefficients:\", true_coef)\n",
    "# Plot the data\n",
    "plt.scatter(X, y, color=\"blue\", label=\"data\")\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.title(\"Linear Regression Data\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94946fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_b = np.c_[np.ones((len(X),1)), X]\n",
    "theta = np.linalg.inv(X_b.T @ X_b) @ X_b.T @ y\n",
    "print(\"True coef:\", true_coef)\n",
    "print(\"Closed-form coef:\", theta[1])\n",
    "print(\"Intercept:\", theta[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9edda52",
   "metadata": {},
   "source": [
    "The coefficients above are computed using the closed-form equation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "698983d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_grid = np.linspace(X.min(), X.max(), 100).reshape(-1,1)\n",
    "y_pred = theta[1]*x_grid + theta[0]\n",
    "\n",
    "plt.scatter(X, y, color=\"blue\", label=\"data\")\n",
    "plt.plot(x_grid, y_pred, color=\"red\", label=\"fit\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "147fefed",
   "metadata": {},
   "source": [
    "### scikit-learn linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3392f00",
   "metadata": {},
   "source": [
    "`LinearRegression` in scikit-learn solves the same least-squares problem for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e88d6c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression()\n",
    "model.fit(X, y)\n",
    "print(\"sklearn coef:\", model.coef_[0])\n",
    "print(\"sklearn intercept:\", model.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f29f76",
   "metadata": {},
   "source": [
    "## 1. k-NN classification from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a8ba10",
   "metadata": {},
   "source": [
    "k-Nearest Neighbors (k-NN) is a simple, intuitive algorithm for classification and regression. For a new sample, k-NN finds the $k$ closest points in the training dataâ€”using a distance metric such as Euclidean distance:\n",
    "\n",
    "$$\n",
    "d(x_i, x_j) = \\sqrt{\\sum_{k=1}^d (x_{i,k} - x_{j,k})^2}\n",
    "$$\n",
    "\n",
    "and assigns the most common class (for classification) or the average value (for regression) among those neighbors.\n",
    "\n",
    "The k-NN process:\n",
    "\n",
    "1. **Choose $k$:** Number of neighbors.\n",
    "2. **Compute distances:** Measure distance from the test sample to all training samples.\n",
    "3. **Find neighbors:** Select the $k$ closest training samples.\n",
    "4. **Predict:** Use majority vote (classification) or average (regression).\n",
    "\n",
    "k-NN is non-parametric and instance-based, relying on the entire training set for predictions. The choice of $k$ and distance metric affects performance: small $k$ can be sensitive to noise, while large $k$ may smooth out class boundaries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6b8c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = load_digits(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b998f4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn_predict(X_train, y_train, X_test, k=3):\n",
    "    preds = []\n",
    "    for x in X_test:\n",
    "        dists = np.linalg.norm(X_train - x, axis=1)\n",
    "        idx = np.argsort(dists)[:k]\n",
    "        preds.append(np.bincount(y_train[idx]).argmax())\n",
    "    return np.array(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0df7d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = knn_predict(X_train, y_train, X_test, k=3)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f60ba4a",
   "metadata": {},
   "source": [
    "### scikit-learn k-NN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1592088a",
   "metadata": {},
   "source": [
    "`KNeighborsClassifier` performs this neighbor search efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa882dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "knn.fit(X_train, y_train)\n",
    "sk_preds = knn.predict(X_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, sk_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb9a12a",
   "metadata": {},
   "source": [
    "## 2. Perceptron with gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e8908f2",
   "metadata": {},
   "source": [
    "The perceptron algorithm is an iterative method for finding a linear decision boundary between two classes. The steps are:\n",
    "\n",
    "1. **Initialize** the weights $w$ and bias $b$ to zero (or small random values).\n",
    "2. **For each training sample** $(x_i, y_i)$, where $y_i \\in \\{-1, 1\\}$:\n",
    "    - Compute the margin: $m_i = y_i (w^T x_i + b)$.\n",
    "    - If $m_i < 0$ (i.e., the sample is misclassified):\n",
    "      - Update the weights: $w \\leftarrow w + \\eta y_i x_i$\n",
    "      - Update the bias: $b \\leftarrow b + \\eta y_i$\n",
    "    - Here, $\\eta$ is the learning rate (often set to 1).\n",
    "3. **Repeat** over the dataset for a fixed number of epochs or until all samples are correctly classified.\n",
    "\n",
    "The perceptron only updates its parameters when it makes a mistake. This process continues until the algorithm converges (if the data is linearly separable) or the maximum number of iterations is reached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1abbe288",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_classification(n_samples=200, n_features=2, n_informative=2, n_redundant=0, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72c8d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_signed = np.where(y_train==0, -1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57792f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.random.randn(X_train.shape[1])\n",
    "b = 0.\n",
    "learning_rate = 0.1\n",
    "num_iterations = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69096d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(num_iterations):\n",
    "    \n",
    "    margins = y_train_signed * (X_train @ w + b)\n",
    "    mask = margins < 0\n",
    "\n",
    "    if not mask.any():\n",
    "        break\n",
    "    \n",
    "    grad_w = -(y_train_signed[mask,None] * X_train[mask]).mean(axis=0)\n",
    "    grad_b = -(y_train_signed[mask]).mean()\n",
    "    \n",
    "    w -= learning_rate * grad_w\n",
    "    b -= learning_rate * grad_b\n",
    "\n",
    "preds = (X_test @ w + b >= 0).astype(int)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e06fce",
   "metadata": {},
   "source": [
    "### scikit-learn Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f67f53e",
   "metadata": {},
   "source": [
    "scikit-learn's `Perceptron` implements the same algorithm with optimizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "829b14ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "skp = Perceptron(max_iter=1000, eta0=0.1, tol=1e-3)\n",
    "skp.fit(X_train, y_train)\n",
    "sk_preds = skp.predict(X_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, sk_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d522c6",
   "metadata": {},
   "source": [
    "## 3. Logistic regression from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "126f04e9",
   "metadata": {},
   "source": [
    "Logistic regression is a linear model for binary classification. It predicts the probability that input $x$ belongs to class 1 using the sigmoid function:\n",
    "\n",
    "$$\n",
    "p(y=1|x) = \\sigma(w^T x + b) = \\frac{1}{1 + e^{-(w^T x + b)}}\n",
    "$$\n",
    "\n",
    "where $w$ is the weight vector, $b$ is the bias, and $\\sigma(z)$ is the sigmoid.\n",
    "\n",
    "**Training steps:**\n",
    "\n",
    "1. Initialize $w$ and $b$.\n",
    "2. Compute predicted probabilities: $\\hat{y} = \\sigma(w^T x + b)$.\n",
    "3. Compute binary cross-entropy loss:\n",
    "\n",
    "    $$\n",
    "    L = -\\frac{1}{n} \\sum_{i=1}^n \\left[ y_i \\log \\hat{y}_i + (1-y_i) \\log (1-\\hat{y}_i) \\right]\n",
    "    $$\n",
    "\n",
    "4. Compute gradients:\n",
    "\n",
    "    $$\n",
    "    \\frac{\\partial L}{\\partial w} = \\frac{1}{n} X^T (\\hat{y} - y), \\quad\n",
    "    \\frac{\\partial L}{\\partial b} = \\frac{1}{n} \\sum_{i=1}^n (\\hat{y}_i - y_i)\n",
    "    $$\n",
    "\n",
    "5. Update $w$ and $b$ with gradient descent.\n",
    "6. Repeat until convergence.\n",
    "\n",
    "Classify as 1 if $p(y=1|x) \\geq 0.5$, else 0.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0134d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = load_iris(return_X_y=True)\n",
    "mask = y < 2\n",
    "X = X[mask, :2]\n",
    "y = y[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "985e2a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "w = np.zeros(X_train.shape[1])\n",
    "b = 0.\n",
    "num_iterations = 200\n",
    "learning_rate = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1782b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1/(1+np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e5989f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(num_iterations):\n",
    "    z = X_train @ w + b\n",
    "    preds = sigmoid(z)\n",
    "    grad_w = X_train.T @ (preds - y_train) / len(y_train)\n",
    "    grad_b = np.mean(preds - y_train)\n",
    "    w -= learning_rate * grad_w\n",
    "    b -= learning_rate * grad_b\n",
    "\n",
    "preds = (sigmoid(X_test @ w + b) >= 0.5).astype(int)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64bb3178",
   "metadata": {},
   "source": [
    "### scikit-learn logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b049782",
   "metadata": {},
   "source": [
    "scikit-learn's `LogisticRegression` uses regularization and a reliable solver to fit the same model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddbbfdfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression(max_iter=200)\n",
    "clf.fit(X_train, y_train)\n",
    "sk_preds = clf.predict(X_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, sk_preds))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cogs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
